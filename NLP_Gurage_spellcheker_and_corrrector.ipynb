{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aronsinkie/NIDS/blob/master/NLP_Gurage_spellcheker_and_corrrector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rena-I0opc-o"
      },
      "source": [
        "#Mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AXcvTamb0Cs",
        "outputId": "47b5a4d4-fd52-4775-d273-0b8e9c319103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlegbBLS_49L"
      },
      "source": [
        "# ·âµ·åç·â†·à´·ãé·âΩ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiiCJY2UArgH"
      },
      "source": [
        "## ·å•·âÖ·àç"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMVukaJBi9Of",
        "outputId": "5d53cfde-47c9-4f41-c7e8-0c9654295756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ],
      "source": [
        "pip install colorama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBi2opVBqJu9"
      },
      "source": [
        "# ·âÖ·å•·ã´·äï ·àõ·àµ·ãé·åà·ãµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4nFsnovkUwp"
      },
      "outputs": [],
      "source": [
        "def remove_amharic_noun_affixes(word):\n",
        "    for prefix in amharic_noun_prefixes:\n",
        "        if word.startswith(prefix):\n",
        "            word = word[len(prefix):]\n",
        "            break\n",
        "    for suffix in amharic_noun_suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            word = word[:-len(suffix)]\n",
        "            break\n",
        "    return word#.rstrip(word[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed98eKBsndE3"
      },
      "outputs": [],
      "source": [
        "def remove_amharic_verb_affixes(word):\n",
        "    for prefix in amharic_verb_prefixes:\n",
        "        if word.startswith(prefix):\n",
        "            word = word[len(prefix):]\n",
        "            break\n",
        "    for suffix in amharic_verb_suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            word = word[:-len(suffix)]\n",
        "            break\n",
        "    return word#.rstrip(word[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNUPR2YWyN5m"
      },
      "outputs": [],
      "source": [
        "def remove_amharic_adj_affixes(word):\n",
        "    for prefix in amharic_adj_prefixes:\n",
        "        if word.startswith(prefix):\n",
        "            word = word[len(prefix):]\n",
        "            break\n",
        "    for suffix in amharic_adj_suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            word = word[:-len(suffix)]\n",
        "            break\n",
        "    return word#.rstrip(word[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbGztI1ipzeY"
      },
      "source": [
        "# ·â∞·äê·â£·â¢ ·ãê·äì·â£·â¢"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGtXiPr6jray"
      },
      "outputs": [],
      "source": [
        "RIBI = {\n",
        "'·àê':'·àï·ä†',\n",
        "'·àë':'·àï·ä°',\n",
        "'·àí':'·àï·ä¢',\n",
        "'·àì':'·àï·ä£',\n",
        "'·àî':'·àï·ä§',\n",
        "'·àï':'·àï',\n",
        "'·àñ':'·àï·ä¶',\n",
        "'ûü®':'ûü´·ä†',\n",
        "'·àë':'·àï·ä°',\n",
        "'ûü©':'ûü´·ä¢',\n",
        "'·àó':'ûü´·ä£',\n",
        "'ûü™':'ûü´·ä§',\n",
        "'ûü´':'ûü´',\n",
        "'·àñ':'·àï·ä¶',\n",
        "'ûü†':'ûü•·ä†',\n",
        "'ûü°':'ûü•·ä°',\n",
        "'ûü¢':'ûü•·ä¢',\n",
        "'ûü£':'ûü•·ä£',\n",
        "'ûü§':'ûü•·ä§',\n",
        "'ûü•':'ûü•',\n",
        "'ûü¶':'ûü•·ä¶',\n",
        "'·àà':'·àç·ä†',\n",
        "'·àâ':'·àç·ä°',\n",
        "'·àä':'·àç·ä¢',\n",
        "'·àã':'·àç·ä£',\n",
        "'·àå':'·àç·ä§',\n",
        "'·àç':'·àç',\n",
        "'·àé':'·àç·ä¶',\n",
        "'·àò':'·àù·ä†',\n",
        "'·àô':'·àù·ä°',\n",
        "'·àö':'·àù·ä¢',\n",
        "'·àõ':'·àù·ä£',\n",
        "'·àú':'·àù·ä§',\n",
        "'·àù':'·àù',\n",
        "'·àû':'·àù·ä¶',\n",
        "'·éÄ':'·éÉ·ä†',\n",
        "'·àô':'·àù·ä°',\n",
        "'ûü≠':'·éÉ·ä¢',\n",
        "'·àü':'·éÉ·ä£',\n",
        "'ûüÆ':'·éÉ·ä§',\n",
        "'·éÉ':'·éÉ',\n",
        "'·àû':'·àù·ä¶',\n",
        "'·à®':'·à≠·ä†',\n",
        "'·à©':'·à≠·ä°',\n",
        "'·à™':'·à≠·ä¢',\n",
        "'·à´':'·à≠·ä£',\n",
        "'·à¨':'·à≠·ä§',\n",
        "'·à≠':'·à≠',\n",
        "'·àÆ':'·à≠·ä¶',\n",
        "'·à∞':'·àµ·ä†',\n",
        "'·à±':'·àµ·ä°',\n",
        "'·à≤':'·àµ·ä¢',\n",
        "'·à≥':'·àµ·ä£',\n",
        "'·à¥':'·àµ·ä§',\n",
        "'·àµ':'·àµ',\n",
        "'·à∂':'·àµ·ä¶',\n",
        "'·à∏':'·àΩ·ä†',\n",
        "'·àπ':'·àΩ·ä°',\n",
        "'·à∫':'·àΩ·ä¢',\n",
        "'·àª':'·àΩ·ä£',\n",
        "'·àº':'·àΩ·ä§',\n",
        "'·àΩ':'·àΩ',\n",
        "'·àæ':'·àΩ·ä¶',\n",
        "'·âÄ':'·âÖ·ä†',\n",
        "'·âÅ':'·âÖ·ä°',\n",
        "'·âÇ':'·âÖ·ä¢',\n",
        "'·âÉ':'·âÖ·ä£',\n",
        "'·âÑ':'·âÖ·ä§',\n",
        "'·âÖ':'·âÖ',\n",
        "'·âÜ':'·âÖ·ä¶',\n",
        "'·âà':'·âÖ·ä†',\n",
        "'·âÅ':'·âÖ·ä°',\n",
        "'ûü∞':'ûü≤·ä¢',\n",
        "'·âã':'ûü≤·ä£',\n",
        "'ûü±':'ûü≤·ä§',\n",
        "'ûü≤':'ûü≤',\n",
        "'·âÜ':'·âÖ·ä¶',\n",
        "'·âê':'·âï·ä†',\n",
        "'·âë':'·âï·ä°',\n",
        "'·âí':'·âï·ä¢',\n",
        "'·âì':'·âï·ä£',\n",
        "'·âî':'·âï·ä§',\n",
        "'·âï':'·âï',\n",
        "'·âñ':'·âï·ä¶',\n",
        "'·â†':'·â•·ä†',\n",
        "'·â°':'·â•·ä°',\n",
        "'·â¢':'·â•·ä¢',\n",
        "'·â£':'·â•·ä£',\n",
        "'·â§':'·â•·ä§',\n",
        "'·â•':'·â•',\n",
        "'·â¶':'·â•·ä¶',\n",
        "'·éÑ':'·éá·ä†',\n",
        "'·â°':'·â•·ä°',\n",
        "'ûü≥':'ûü≥·ä¢',\n",
        "'·âß':'·éá·ä£',\n",
        "'ûü¥':'·éá·ä§',\n",
        "'·éá':'·éá',\n",
        "'·â¶':'·â•·ä¶',\n",
        "'·â∞':'·âµ·ä†',\n",
        "'·â±':'·âµ·ä°',\n",
        "'·â≤':'·âµ·ä¢',\n",
        "'·â≥':'·âµ·ä£',\n",
        "'·â¥':'·âµ·ä§',\n",
        "'·âµ':'·âµ',\n",
        "'·â∂':'·âµ·ä¶',\n",
        "'·â∏':'·âΩ·ä†',\n",
        "'·âπ':'·âΩ·ä°',\n",
        "'·â∫':'·âΩ·ä¢',\n",
        "'·âª':'·âΩ·ä£',\n",
        "'·âº':'·âΩ·ä§',\n",
        "'·âΩ':'·âΩ',\n",
        "'·âæ':'·âΩ·ä¶',\n",
        "'·äê':'·äï·ä†',\n",
        "'·äë':'·äï·ä°',\n",
        "'·äí':'·äï·ä¢',\n",
        "'·äì':'·äï·ä£',\n",
        "'·äî':'·äï·ä§',\n",
        "'·äï':'·äï',\n",
        "'·äñ':'·äï·ä¶',\n",
        "'·äò':'·äù·ä†',\n",
        "'·äô':'·äù·ä°',\n",
        "'·äö':'·äù·ä¢',\n",
        "'·äõ':'·äù·ä£',\n",
        "'·äú':'·äù·ä§',\n",
        "'·äù':'·äù',\n",
        "'·äû':'·äù·ä¶',\n",
        "'·ä†':'·ä†',\n",
        "'·ä°':'·ä°',\n",
        "'·ä¢':'·ä¢',\n",
        "'·ä£':'·ä£',\n",
        "'·ä§':'·ä§',\n",
        "'·ä•':'·ä•',\n",
        "'·ä¶':'·ä¶',\n",
        "'·ä®':'·ä≠·ä†',\n",
        "'·ä©':'·ä≠·ä°',\n",
        "'·ä™':'·ä≠·ä¢',\n",
        "'·ä´':'·ä≠·ä£',\n",
        "'·ä¨':'·ä≠·ä§',\n",
        "'·ä≠':'·ä≠·ä•',\n",
        "'·äÆ':'·ä≠·ä¶',\n",
        "'·ä∞':'ûü∑·ä†',\n",
        "'·ä©':'·ä≠·ä°',\n",
        "'ûüµ':'ûü∑·ä¢',\n",
        "'·ä≥':'ûü∑·ä£',\n",
        "'ûü∂':'ûü∑·ä§',\n",
        "'ûü∑':'ûü∑',\n",
        "'·äÆ':'·ä≠·ä¶',\n",
        "'·ä∏':'·äΩ·ä†',\n",
        "'·äπ':'·äΩ·ä°',\n",
        "'·ä∫':'·äΩ·ä¢',\n",
        "'·äª':'·äΩ·ä£',\n",
        "'·äº':'·äΩ·ä§',\n",
        "'·äΩ':'·äΩ',\n",
        "'·äæ':'·äΩ·ä¶',\n",
        "'·ãà':'·ãç·ä†',\n",
        "'·ãâ':'·ãç·ä°',\n",
        "'·ãä':'·ãç·ä¢',\n",
        "'·ãã':'·ãç·ä£',\n",
        "'·ãå':'·ãç·ä§',\n",
        "'·ãç':'·ãç',\n",
        "'·ãé':'·ãç·ä¶',\n",
        "'·ãê':'·ãï·ä†',\n",
        "'·ãë':'·ãï·ä°',\n",
        "'·ãí':'·ãï·ä¢',\n",
        "'·ãì':'·ãï·ä£',\n",
        "'·ãî':'·ãï·ä§',\n",
        "'·ãï':'·ãï',\n",
        "'·ãñ':'·ãï·ä¶',\n",
        "'·ãò':'·ãù·ä†',\n",
        "'·ãô':'·ãù·ä°',\n",
        "'·ãö':'·ãù·ä¢',\n",
        "'·ãõ':'·ãù·ä£',\n",
        "'·ãú':'·ãù·ä§',\n",
        "'·ãù':'·ãù',\n",
        "'·ãû':'·ãù·ä¶',\n",
        "'·ã†':'·ã•·ä†',\n",
        "'·ã°':'·ã•·ä°',\n",
        "'·ã¢':'·ã•·ä¢',\n",
        "'·ã£':'·ã•·ä£',\n",
        "'·ã§':'·ã•·ä§',\n",
        "'·ã•':'·ã•·ä•',\n",
        "'·ã¶':'·ã•·ä¶',\n",
        "'·ã®':'·ã≠·ä†',\n",
        "'·ã©':'·ã©·ä°',\n",
        "'·ã™':'·ã≠·ä¢',\n",
        "'·ã´':'·ã≠·ä£',\n",
        "'·ã¨':'·ã≠·ä§',\n",
        "'·ã≠':'·ã≠',\n",
        "'·ãÆ':'·ã≠·ä¶',\n",
        "'·ã∞':'·ãµ·ä†',\n",
        "'·ã±':'·ãµ·ä°',\n",
        "'·ã≤':'·ãµ·ä¢',\n",
        "'·ã≥':'·ãµ·ä£',\n",
        "'·ã¥':'·ãµ·ä§',\n",
        "'·ãµ':'·ãµ',\n",
        "'·ã∂':'·ãµ·ä¶',\n",
        "'·åÄ':'·åÖ·ä†',\n",
        "'·åÅ':'·åÖ·ä°',\n",
        "'·åÇ':'·åÖ·ä¢',\n",
        "'·åÉ':'·åÖ·ä£',\n",
        "'·åÑ':'·åÖ·ä§',\n",
        "'·åÖ':'·åÖ',\n",
        "'·åÇ':'·åÖ·ä¶',\n",
        "'·åà':'·åç·ä†',\n",
        "'·åâ':'·åç·ä°',\n",
        "'·åä':'·åç·ä¢',\n",
        "'·åã':'·åç·ä£',\n",
        "'·åå':'·åç·ä§',\n",
        "'·åç':'·åç·ä•',\n",
        "'·åé':'·åç·ä¶',\n",
        "'·åê':'·åç·ä†',\n",
        "'·åâ':'·åç·ä°',\n",
        "'ûü∏':'ûü∫·ä¢',\n",
        "'·åì':'ûü∫·ä£',\n",
        "'ûüπ':'ûü∫·ä§',\n",
        "'ûü∫':'ûü∫',\n",
        "'·åé':'·åà·ä¶',\n",
        "'·åò':'·åù·ä†',\n",
        "'·åô':'·åù·ä°',\n",
        "'·åö':'·åù·ä¢',\n",
        "'·åõ':'·åù·ä£',\n",
        "'·åú':'·åù·ä§',\n",
        "'·åù':'·åù',\n",
        "'·åû':'·åù·ä¶',\n",
        "'·å†':'·å•·ä†',\n",
        "'·å°':'·å•·ä°',\n",
        "'·å¢':'·å•·ä¢',\n",
        "'·å£':'·å•·ä£',\n",
        "'·å§':'·å•·ä§',\n",
        "'·å•':'·å•',\n",
        "'·å¶':'·å•·ä¶',\n",
        "'·å®':'·å≠·ä†',\n",
        "'·å©':'·å≠·ä°',\n",
        "'·å™':'·å≠·ä¢',\n",
        "'·å´':'·å≠·ä£',\n",
        "'·å¨':'·å≠·ä§',\n",
        "'·å≠':'·å≠·ä•',\n",
        "'·åÆ':'·å≠·ä¶',\n",
        "'·çà':'·çç·ä†',\n",
        "'·çâ':'·çç·ä°',\n",
        "'·çä':'·çç·ä¢',\n",
        "'·çã':'·çç·ä£',\n",
        "'·çå':'·çç·ä§',\n",
        "'·çç':'·çç',\n",
        "'·çé':'·çç·ä¶',\n",
        "'·éà':'·éã·ä†',\n",
        "'·çâ':'·éã·ä°',\n",
        "'ûüª':'·éã·ä¢',\n",
        "'·çè':'·éã·ä£',\n",
        "'ûüº':'·éã·ä§',\n",
        "'·éã':'·éã',\n",
        "'·çé':'·çç·ä¶',\n",
        "'·çê':'·çï·ä†',\n",
        "'·çë':'·çï·ä°',\n",
        "'·çí':'·çï·ä¢',\n",
        "'·çì':'·çï·ä£',\n",
        "'·çî':'·çï·ä§',\n",
        "'·çï':'·çï·ä•',\n",
        "'·çñ':'·çï·ä¶',\n",
        "'·éå':'·éè·ä†',\n",
        "'·çë':'·çï·ä°',\n",
        "'ûüΩ':'·éè·ä¢',\n",
        "'ûüæ':'·éè·ä§',\n",
        "'·éè':'·éè',\n",
        "'·çì':'·çï·ä¶',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bRO151zkz3g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzu-T7zIvmW0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GATlxdAmqXGE"
      },
      "source": [
        "# ·ãã·äì ·âÉ·àç ·àù·àµ·à®·â≥(so_called_steam_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDdcWgdQlP3k"
      },
      "outputs": [],
      "source": [
        "def so_called_steam_noun_word(word):\n",
        "    for prefix in amharic_noun_prefixes:\n",
        "        if word.startswith(prefix):\n",
        "            word = word[len(prefix):]\n",
        "            break\n",
        "    for suffix in amharic_noun_suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            word = word[:-len(suffix)]\n",
        "            break\n",
        "        word = word.replace(\"-\", \"\")\n",
        "    return word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JsTWrGmhww3"
      },
      "outputs": [],
      "source": [
        "def so_called_steam_verb_word(word):\n",
        "    for prefix in amharic_verb_prefixes:\n",
        "        if word.startswith(prefix):\n",
        "            word = word[len(prefix):]\n",
        "            break\n",
        "    for suffix in amharic_verb_suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            word = word[:-len(suffix)]\n",
        "            break\n",
        "        word = word.replace(\"-\", \"\")\n",
        "    return word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upWMESWMhdDN"
      },
      "outputs": [],
      "source": [
        "def so_called_steam_adj_word(word):\n",
        "    for prefix in amharic_adj_prefixes:\n",
        "        if word.startswith(prefix):\n",
        "            word = word[len(prefix):]\n",
        "            break\n",
        "    for suffix in amharic_adj_suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            word = word[:-len(suffix)]\n",
        "            break\n",
        "        word = word.replace(\"-\", \"\")\n",
        "    return word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt7lGInCqlmp"
      },
      "source": [
        "# ·â∞·äê·â£·â¢ ·ä†·äì·â£·â¢ ·âµ·äï·âµ·äì ·âµ·åç·â†·à´(Erbata_TIGBERA and reverse_Erbata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAAJeoS4lb7R"
      },
      "outputs": [],
      "source": [
        "def Erbata_TIGBERA(KAL: str, RIBI: dict) -> str:\n",
        "    Anababi_Erbata = [RIBI.get(FIDEL, '') for FIDEL in KAL]\n",
        "    Anababi_Erbata = '-'.join(Anababi_Erbata)\n",
        "    return Anababi_Erbata\n",
        "def reverse_Erbata(KAL: str, RIBI: dict) -> str:\n",
        "    split_words = KAL.split(\"-\")\n",
        "    new_word = ''\n",
        "    for split_word in split_words:\n",
        "        for key, value in RIBI.items():\n",
        "            if value == split_word:\n",
        "                new_word += key\n",
        "                break\n",
        "        else:\n",
        "            new_word += split_word\n",
        "        new_word += '-'\n",
        "    return new_word.rstrip('-')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdx5asePrdXz"
      },
      "source": [
        "# ·âÉ·àã·âµ·äï ·àõ·à≠·â£·âµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3oXllXRlsRI"
      },
      "outputs": [],
      "source": [
        "def add_amharic_noun_affixes(stem_word):\n",
        "    words_with_affixes = []\n",
        "\n",
        "    for prefix in amharic_noun_prefixes:\n",
        "        words_with_affixes.append(prefix + stem_word)\n",
        "\n",
        "    for suffix in amharic_noun_suffixes:\n",
        "        words_with_affixes.append(stem_word[:-1] + suffix)\n",
        "\n",
        "    return words_with_affixes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdFL4WB2zaVI"
      },
      "outputs": [],
      "source": [
        "def add_amharic_verb_affixes(stem_word):\n",
        "    words_with_affixes = []\n",
        "\n",
        "    for prefix in amharic_verb_prefixes:\n",
        "        words_with_affixes.append(prefix + stem_word)\n",
        "\n",
        "    for suffix in amharic_verb_suffixes:\n",
        "        words_with_affixes.append(stem_word[:-1] + suffix)\n",
        "\n",
        "    return words_with_affixes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OATLjkvnzcbw"
      },
      "outputs": [],
      "source": [
        "def add_amharic_adj_affixes(stem_word):\n",
        "    words_with_affixes = []\n",
        "\n",
        "    for prefix in amharic_adj_prefixes:\n",
        "        words_with_affixes.append(prefix + stem_word)\n",
        "\n",
        "    for suffix in amharic_adj_suffixes:\n",
        "        words_with_affixes.append(stem_word[:-1] + suffix)\n",
        "\n",
        "    return words_with_affixes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPnH4Q0Rrtf-"
      },
      "source": [
        "# ·à∞·à®·ãù·äï ·àõ·å•·çã·âµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjyV9eGYl5PO"
      },
      "outputs": [],
      "source": [
        "def remove_hyphen(word):\n",
        "    return word.replace(\"-\", \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnBcNS3xrzSv"
      },
      "source": [
        "# ·ä®·ãã·äì ·âÖ·àç ·ä®·à®·å¢·âµ ·åã·à≠ ·àõ·àò·à≥·à∞·àç"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIjhunagpWlj"
      },
      "outputs": [],
      "source": [
        "def check_string_in_file(file_path, search_string):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                if search_string.encode('utf-8') in line.encode('utf-8'):\n",
        "                    return True\n",
        "        return False\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File '{file_path}' not found.\")\n",
        "        return False\n",
        "    except IOError:\n",
        "        print(f\"Error reading file '{file_path}'.\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDgSBKg1sQ4n"
      },
      "source": [
        "# ·ä®·ãã·äì ·âÉ·àç ·âÖ·à≠·â• ·ã®·àñ·äê·ãç·äï ·àò·çà·àà·åç"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ilb0a-HPpfP3"
      },
      "outputs": [],
      "source": [
        "def find_closest_word(root_word, misspelled_word):\n",
        "    with open(root_word, \"r\", encoding=\"utf-8\") as file:\n",
        "        root_words = [line.strip() for line in file]\n",
        "\n",
        "    closest_word = difflib.get_close_matches(misspelled_word, root_words, n=1)\n",
        "    if closest_word:\n",
        "        return closest_word[0]\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7ULOWofsjXI"
      },
      "source": [
        "# family and order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiK_9LDssUa3"
      },
      "outputs": [],
      "source": [
        "def family(char1, char2):\n",
        "    char1 = Erbata_TIGBERA(char1, RIBI)\n",
        "    char2 = Erbata_TIGBERA(char2, RIBI)\n",
        "    if len(char1) <  2 and len(char2)< 2:\n",
        "      return 1\n",
        "    elif char1[0] == char2[0]:\n",
        "        return 1\n",
        "    elif len(char1) < 1 or len(char2) < 1:\n",
        "      return 0\n",
        "    return 0\n",
        "def order(char1, char2):\n",
        "\n",
        "        char1 = Erbata_TIGBERA(char1, RIBI)\n",
        "        char2 = Erbata_TIGBERA(char2, RIBI)\n",
        "        if len(char1) >= 2 and len(char2) >= 2:\n",
        "          if char1[1] == char2[1]:\n",
        "            return 1\n",
        "        if len(char1) < 2 and len(char2) >= 2:\n",
        "          if char1[0] == char2[1]:\n",
        "            return 1\n",
        "        if len(char1) >=  2 and len(char2)< 2:\n",
        "          if char1[1] == char2[0]:\n",
        "            return 1\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf_j8vCgtJE9"
      },
      "source": [
        "# ·â∞·àò·à≥·à≥·ã≠ ·àñ·àí·ã´·âµ·äï ·àò·çà·àà·åç"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIbQvd_gsbnA"
      },
      "outputs": [],
      "source": [
        "def match_words(word1, word2):\n",
        "    matched_chars = \"\"\n",
        "    unmatched_chars1_before = \"\"\n",
        "    unmatched_chars1_after = \"\"\n",
        "    unmatched_chars2_before = \"\"\n",
        "    unmatched_chars2_after = \"\"\n",
        "\n",
        "    if word1 is None or word2 is None:\n",
        "        return matched_chars, unmatched_chars1_before, unmatched_chars1_after, unmatched_chars2_before, unmatched_chars2_after\n",
        "\n",
        "    # Convert the words into lists of characters\n",
        "    word1_list = list(word1)\n",
        "    word2_list = list(word2)\n",
        "\n",
        "    # Iterate over each character in word1\n",
        "    for char1 in word1_list:\n",
        "        # Iterate over each character in word2\n",
        "        for char2 in word2_list:\n",
        "            if char1 == char2:\n",
        "                matched_chars += char1\n",
        "\n",
        "    if matched_chars:\n",
        "        # Find the index of the first matched character in word1\n",
        "        first_match_index = word1.index(matched_chars[0])\n",
        "\n",
        "        # Extract the characters before and after the matched characters for word1\n",
        "        unmatched_chars1_before = word1[:first_match_index]\n",
        "        unmatched_chars1_after = word1[first_match_index + len(matched_chars):]\n",
        "\n",
        "        # Find the index of the first matched character in word2\n",
        "        first_match_index = word2.index(matched_chars[0])\n",
        "\n",
        "        # Extract the characters before and after the matched characters for word2\n",
        "        unmatched_chars2_before = word2[:first_match_index]\n",
        "        unmatched_chars2_after = word2[first_match_index + len(matched_chars):]\n",
        "    else:\n",
        "        unmatched_chars1_after = word1\n",
        "        unmatched_chars2_after = word2\n",
        "\n",
        "    return (\n",
        "        matched_chars,\n",
        "        unmatched_chars1_before,\n",
        "        unmatched_chars1_after,\n",
        "        unmatched_chars2_before,\n",
        "        unmatched_chars2_after\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWQ9NU1GtYLc"
      },
      "source": [
        "# ·âÉ·àâ ·ä®·à≠·â¢ ·âÉ·àã·âµ ·åã·à≠ ·ã´·àà·ãç ·à≠·âÄ·âµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMmNPW31smL3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compare_words(word1, word2):\n",
        "    matched, unmatched1_before, unmatched1_after, unmatched_chars2_before, unmatched_chars2_after = match_words(word1, word2)\n",
        "    # Step 3: Computing average distance between unmatched blocks\n",
        "    total_distance = 0.0\n",
        "    comparisons = 0\n",
        "    unmatched_dif2=0\n",
        "    unmatched_dif1=0\n",
        "    XX=0\n",
        "    if unmatched1_after or unmatched_chars2_after:\n",
        "        min_length2 = min(len(unmatched1_after), len(unmatched_chars2_after))\n",
        "        max_length2 = max(len(unmatched1_after), len(unmatched_chars2_after))\n",
        "        unmatched_dif2 = max_length2 - min_length2\n",
        "        for i in range(min_length2):\n",
        "          for j in range(min_length2):\n",
        "                char1 = unmatched1_after[i]\n",
        "                char2 = unmatched_chars2_after[j]\n",
        "\n",
        "                comparisons += 1\n",
        "                if char1 =='' :\n",
        "                  total_distance += 0.1\n",
        "\n",
        "                elif char2 =='':\n",
        "                  total_distance += 0.1\n",
        "\n",
        "                elif family(char1, char2) == 1 and order(char1, char2) == 1:\n",
        "                  total_distance += 1\n",
        "\n",
        "                elif family(char1, char2) == 1 and order(char1, char2) != 1:\n",
        "                  total_distance += 0.7\n",
        "\n",
        "                elif family(char1, char2) != 1 and order(char1, char2) == 1:\n",
        "                  total_distance += 0.5\n",
        "\n",
        "                elif family(char1, char2) != 1 and order(char1, char2) != 1:\n",
        "                    total_distance += 0.3\n",
        "\n",
        "    if unmatched1_before or unmatched_chars2_before:\n",
        "        min_length1 = min(len(unmatched1_before), len(unmatched_chars2_before))\n",
        "        max_length1 = max(len(unmatched1_before), len(unmatched_chars2_before))\n",
        "        unmatched_dif1 = max_length1 - min_length1\n",
        "        for i in range(min_length1):\n",
        "\n",
        "          for j in range(min_length1):\n",
        "                char1 = unmatched1_before[i]\n",
        "                char2 = unmatched_chars2_before[j]\n",
        "                comparisons += 1\n",
        "                if char1 =='' :\n",
        "                  total_distance += 0.1\n",
        "                elif char2 =='':\n",
        "                  total_distance += 0.1\n",
        "\n",
        "                elif family(char1, char2) == 1 and order(char1, char2) == 1:\n",
        "                  total_distance += 1\n",
        "                elif family(char1, char2) == 1 and order(char1, char2) != 1:\n",
        "                  total_distance += 0.7\n",
        "                elif family(char1, char2) != 1 and order(char1, char2) == 1:\n",
        "                  total_distance += 0.5\n",
        "                elif family(char1, char2) != 1 and order(char1, char2) != 1:\n",
        "                    total_distance += 0.3\n",
        "    total_distance += len(matched)\n",
        "    XX=(unmatched_dif1 + unmatched_dif2)\n",
        "    yy=(XX/10)\n",
        "    total_distance =yy+total_distance\n",
        "    total_distance = float(\"{:.1f}\".format(total_distance))\n",
        "    comparisons += unmatched_dif1 + unmatched_dif2+len(matched)\n",
        "    average_distance = total_distance / int(comparisons)\n",
        "    average_distance = \"\\033[91m\" + str(average_distance) + \"\\033[0m\"  # Set the value of word1 to red color\n",
        "    return matched, unmatched1_before, unmatched_chars2_before,unmatched1_after, unmatched_chars2_after, total_distance, comparisons, average_distance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNfRl4ictwZv"
      },
      "source": [
        "# prefix and suffix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OmaK4GDuwoU",
        "outputId": "5cda3c09-ca9a-4c49-a0f9-936c5600b7ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amharic_verb_prefixes: ['·ã≠·ä†-', '·ä£-', '·ä£-·äï·ä£-', '·ä†-', '·ä£-·äï-', '·â•·ä†-', '·â•·ä£-·äï-', '·â•·ä£-', '·â•·ä£-·äï·ä£-', '·âµ·ä£-', '·âµ·ä£-·äï·ä£-', '·âµ·ä†-', '·âµ·ä£-·äï-', '·äï-', '·âµ-', '·ã≠-', '·âµ·ä§-', '·âµ·ä¢-']\n"
          ]
        }
      ],
      "source": [
        "import chardet\n",
        "\n",
        "file_path = '/content/drive/MyDrive/prefixes_repition_removed.txt'\n",
        "\n",
        "# Detect file encoding\n",
        "with open(file_path, 'rb') as file:\n",
        "    raw_data = file.read()\n",
        "    result = chardet.detect(raw_data)\n",
        "    encoding = result['encoding']\n",
        "\n",
        "# Read prefixes from text file\n",
        "with open(file_path, 'r', encoding=encoding) as file:\n",
        "    amharic_verb_prefixes = [line.strip() for line in file]\n",
        "\n",
        "print(\"amharic_verb_prefixes:\", amharic_verb_prefixes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYNTr0MQuzCS",
        "outputId": "5ef88a5a-5e81-4566-d7b3-12f3856b957d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amharic_verb_suffixes: ['·ä†', '·ä†-\\U0001e7eb-·àù', '·ä†-·äï·ä†-·àù', '·ä†-·àï·ä†-·àù', '·ä†-·àï·ä°-·àù', '·ä†-\\U0001e7e5-·àù', '·ä†-·àï-·àù·ä£-·àù', '·ä†-·ä†-·àù', '·ä†-·àù', '·ä†-·ä¶-·àù', '·ä†-·âΩ-·àù', '·ä†-·àù·ä£-·àù', '·ä†-·äï-·ä≠·ä†-·àù', '·ä†-·äï-·ä≠·ä°-·àù', '·ä†-·äï-·äΩ-·àù', '·ä†-·äï-·ä≠·ä•-·àù·ä£-·àù', '·ä†-\\U0001e7eb-·äï·ä†-·àù', '·ä†-·àï-·äï·ä¶-·àù', '·ä†-\\U0001e7eb-·äï·ä£-·àù', '·ä†-·àï-·äï·ä†-·àù·ä£-·àù', '·ä†-·äï·ä§-·äï·ä¶-·àù', '·ä†-·äï·ä§-·äï-·ä≠·ä†-·àù', '·ä†-·äï·ä§-·äï-·ä≠·ä°-·àù', '·ä†-·äï·ä§-·äï-·äΩ-·àù', '·ä†-·ä†-·äï·ä§-·äï-·ä≠·ä•-·àù·ä£-·àù', '·ä†-·äï·ä§-·äï·ä†-·àù', '·ä†-·äï·ä§-·äï·ä£-·àù', '·ä†-·äï·ä§-·äï·ä†-·àù·ä£-·àù', '·ä†-·àï·ä†-·äï·ä¢-·àù', '·ä†-·àï·ä†-·äï-·ãµ·ä†-·àù', '·ä†-·àï·ä†-·äï-·àï·ä°-·àù', '·ä†-·àï·ä†-·äï-\\U0001e7e5-·àù', '·ä†-·àï·ä†-·äï-·àï-·àù·ä£-·àù', '·ä†-\\U0001e7eb·ä†-·äï·ä†-·àù', '·ä†-·àï·ä†-·äï·ä¶-·àù', '·ä†-·àï·ä†-·äï·ä£-·àù', '·ä†-·àï·ä†-·äï·ä†-·àù·ä£-·àù', '·ä†-·àï·ä°-·äï·ä¢-·àù', '·ä†-·àï·ä°-·äï-·ãµ·ä†-·àù', '·ä†-·àï·ä°-·äï-·ä≠·ä†-·àù', '·ä†-·àï·ä°-·äï-·ä≠·ä°-·àù', '·ä†-·àï·ä°-·äï-·äΩ-·àù', '·ä†-·àï·ä°-·äï-·ä≠·ä•-·àù·ä£-·àù', '·ä†-·àï·ä°-·äï·ä†-·àù', '·ä†-·àï·ä°-·äï·ä¶-·àù', '·ä†-·àï·ä°-·äï·ä£-·àù', '·ä†-·àï·ä°-·äï·ä†-·àù·ä£-·àù', '·ä†-·äï·ä¢-·àù', '·ä†-·äï-·ãµ·ä†-·àù', '·ä†-·äï-·àï·ä†-·àù', '·ä†-·äï-·àï·ä°-·àù', '·ä†-·äï-\\U0001e7e5-·àù', '·ä†-·äï-·àï-·àù·ä£-·àù', '·ä†-·äï·ä¶-·àù', '·ä†-·äï·ä£-·àù', '·ä†-·äï·ä†-·àù·ä£-·àù', '·ä†-·ä¶-·äï·ä¢-·àù', '·ä†-·ä¶-·äï-·ãµ·ä†-·àù', '·ä†-·ä¶-·äï-·ä≠·ä†-·àù', '·ä†-·ä¶-·äï-·ä≠·ä°-·àù', '·ä†-·ä¶-·äï-·äΩ-·àù', '·ä†-·ä¶-·äï-·ä≠·ä•-·àù·ä£-·àù', '·ä†-·ä¶-·äï·ä†-·àù', '·ä†-·ä¶-·äï·ä¶-·àù', '·ä†-·ä¶-·äï·ä£-·àù', '·ä†-·ä¶-·äï·ä†-·àù·ä£-·àù', '·ä†-·âΩ·ä†-·äï·ä¢-·àù', '·ä†-·âΩ·ä†-·äï·ä†-·ãµ·ä†-·àù', '·ä†-·âΩ·ä†-·äï·ä†-·àï·ä†-·àù', '·ä†-·âΩ·ä†-·äï·ä†-·àï·ä°-·àù', '·ä†-·âΩ·ä†-·äï·ä†-\\U0001e7e5-·àù', '·ä†-·âΩ·ä†-·äï·ä†-·àï-·àù·ä£-·àù', '·ä†-·âΩ·ä†-·äï·ä†-·àù', '·ä†-·âΩ·ä†-·äï·ä¶-·àù', '·ä†-·âΩ·ä†-·äï·ä£-·àù', '·ä†-·âΩ·ä†-·äï·ä†-·àù·ä£-·àù', '·ä†-·àù·ä£-·äï·ä¢-·àù', '·ä†-·àù·ä£-·äï-·ãµ·ä†-·àù', '·ä†-·àù·ä£-·äï-·àï·ä†-·àù', '·ä†-·àù·ä£-·äï-·àï·ä°-·àù', '·ä†-·àù·ä£-·äï-\\U0001e7e5-·àù', '·ä†-·àù·ä£-·äï-·àï-·àù·ä£-·àù', '·ä†-·àù·ä£-·äï·ä†-·àù', '·ä†-·àù·ä£-·äï·ä¶-·àù', '·ä†-·àù·ä£-·äï·ä£-·àù', '·ä†-·àù·ä£-·äï·ä†-·àù·ä£-·àù', '·ä†-·â•-·ä¢-·àù', '·ä†-·â•-·äï-·ãµ·ä†-·àù', '·ä†-·â•-·àï·ä†-·àù', '·ä†-·â•-·àï·ä°-·àù', '·ä†-·â•-\\U0001e7e5-·àù', '·ä†-·â•-·ä†-·àï-·àù·ä£-·àù', '·ä†-·ãç·ä†-·àù', '·ä†-·â•-·ä¶-·àù', '·ä†-·â•-·ä£-·àù', '·ä†-·â•-·àï-·àù·ä£-·àù', '·ä†-·ä¶-\\U0001e7f7·ä¢-·àù', '·ä†-·ä¶-\\U0001e7f7-·äï-·ãµ·ä†-·àù', '·ä†-·ä¶-\\U0001e7f7·ä†-·àù', '·ä†-·ä¶-·ä≠·ä¶-·àù', '·ä†-·ä¶-·äΩ-·àù', '·ä†-·ä¶-·ä≠·ä•-·àù·ä£-·àù', '·ä†-·ä¶-·ä≠·ä†-·àù', '·ä†-·ä¶-\\U0001e7f7·ä£-·àù', '·ä†-·ä¶-·ä≠·ä†-·àù·ä£-·àù', '·ä†-\\U0001e7eb', '·ä†-·äï·ä†', '·ä†-·àï·ä†', '·ä†-·àï·ä°', '·ä†-\\U0001e7e5', '·ä†-·àï-·àù·ä£', '·ä†-·ä¶', '·ä†-·âΩ', '·ä†-·àù·ä£', '·ä†-·äï-·ä≠·ä†', '·ä†-·äï-·ä≠·ä°', '·ä†-·äï-·äΩ', '·ä†-·äï-·ä≠·ä•-·àù·ä£', '·ä†-\\U0001e7eb-·äï·ä†', '·ä†-·àï-·äï·ä¶', '·ä†-\\U0001e7eb-·äï·ä£', '·ä†-·àï-·äï·ä†-·àù·ä£', '·ä†-·äï·ä§-·äï·ä¶', '·ä†-·äï·ä§-·äï-·ä≠·ä†', '·ä†-·äï·ä§-·äï-·ä≠·ä°', '·ä†-·äï·ä§-·äï-·äΩ', '·ä†--·ä†-·äï·ä§-·äï-·ä≠·ä•-·àù·ä£', '·ä†-·äï·ä§-·äï·ä†', '·ä†-·äï·ä§-·äï·ä£', '·ä†-·äï·ä§-·äï·ä†-·àù·ä£', '·ä†-·àï·ä†-·äï·ä¢', '·ä†-·àï·ä†-·äï-·ãµ·ä†', '·ä†-·àï·ä†-·äï-·àï·ä°', '·ä†-·àï·ä†-·äï-\\U0001e7e5', '·ä†-·àï·ä†-·äï-·àï-·àù·ä£', '·ä†-\\U0001e7eb·ä†-·äï·ä†', '·ä†-·àï·ä†-·äï·ä¶', '·ä†-·àï·ä†-·äï·ä£', '·ä†-·àï·ä†-·äï·ä†-·àù·ä£', '·ä†-·àï·ä°-·äï·ä¢', '·ä†-·àï·ä°-·äï-·ãµ·ä†', '·ä†-·àï·ä°-·äï-·ä≠·ä†', '·ä†-·àï·ä°-·äï-·ä≠·ä°', '·ä†-·àï·ä°-·äï-·äΩ', '·ä†-·àï·ä°-·äï-·ä≠·ä•-·àù·ä£', '·ä†-·àï·ä°-·äï·ä†', '·ä†-·àï·ä°-·äï·ä¶', '·ä†-·àï·ä°-·äï·ä£', '·ä†-·àï·ä°-·äï·ä†-·àù·ä£', '·ä†-·äï·ä¢', '·ä†-·äï-·ãµ·ä†', '·ä†-·äï-·àï·ä†', '·ä†-·äï-·àï·ä°', '·ä†-·äï-\\U0001e7e5', '·ä†-·äï-·àï-·àù·ä£', '·ä†-·äï·ä¶', '·ä†-·äï·ä£', '·ä†-·äï·ä†-·àù·ä£', '·ä†-·ä¶-·äï·ä¢', '·ä†-·ä¶-·äï-·ãµ·ä†', '·ä†-·ä¶-·äï-·ä≠·ä†', '·ä†-·ä¶-·äï-·ä≠·ä°', '·ä†-·ä¶-·äï-·äΩ', '·ä†-·ä¶-·äï-·ä≠·ä•-·àù·ä£', '·ä†-·ä¶-·äï·ä†', '·ä†-·ä¶-·äï·ä¶', '·ä†-·ä¶-·äï·ä£', '·ä†-·ä¶-·äï·ä†-·àù·ä£', '·ä†-·âΩ·ä†-·äï·ä¢', '·ä†-·âΩ·ä†-·äï·ä†-·ãµ·ä†', '·ä†-·âΩ·ä†-·äï·ä†-·àï·ä†', '·ä†-·âΩ·ä†-·äï·ä†-·àï·ä°', '·ä†-·âΩ·ä†-·äï·ä†-\\U0001e7e5', '·ä†-·âΩ·ä†-·äï·ä†-·àï-·àù·ä£', '·ä†-·âΩ·ä†-·äï·ä†', '·ä†-·âΩ·ä†-·äï·ä¶', '·ä†-·âΩ·ä†-·äï·ä£', '·ä†-·âΩ·ä†-·äï·ä†-·àù·ä£', '·ä†-·àù·ä£-·äï·ä¢', '·ä†-·àù·ä£-·äï-·ãµ·ä†', '·ä†-·àù·ä£-·äï-·àï·ä†', '·ä†-·àù·ä£-·äï-·àï·ä°', '·ä†-·àù·ä£-·äï-\\U0001e7e5', '·ä†-·àù·ä£-·äï-·àï-·àù·ä£', '·ä†-·àù·ä£-·äï·ä†', '·ä†-·àù·ä£-·äï·ä¶', '·ä†-·àù·ä£-·äï·ä£', '·ä†-·àù·ä£-·äï·ä†-·àù·ä£', '·ä†-·â•-·ä¢', '·ä†-·â•-·äï-·ãµ·ä†', '·ä†-·â•-·àï·ä†', '·ä†-·â•-·àï·ä°', '·ä†-·â•-\\U0001e7e5', '·ä†-·â•-·ä†-·àï-·àù·ä£', '·ä†-·ãç·ä†', '·ä†-·â•-·ä¶', '·ä†-·â•-·ä£', '·ä†-·â•-·àï-·àù·ä£', '·ä†-·ä¶-\\U0001e7f7·ä¢', '·ä†-·ä¶-\\U0001e7f7-·äï-·ãµ·ä†', '·ä†-·ä¶-\\U0001e7f7·ä†', '·ä†-·ä¶-·ä≠·ä¶', '·ä†-·ä¶-·äΩ', '·ä†-·ä¶-·ä≠·ä•-·àù·ä£', '·ä†-·ä¶-·ä≠·ä†', '·ä†-·ä¶-\\U0001e7f7·ä£', '·ä†-·ä¶-·ä≠·ä†-·àù·ä£', '·ä†--·ä†-·äï·ä§-·äï-·ä≠·ä•-·àù·ä£-·àù', '·ä†-\\U0001e7eb-·àù-·âµ·ä£', '·ä†-·äï·ä†-·àù-·âµ·ä£', '·ä†-·àï·ä†-·àù-·âµ·ä£', '·ä†-·àï·ä°-·àù-·âµ·ä£', '·ä†-\\U0001e7e5-·àù-·âµ·ä£', '·ä†-·àï-·àù·ä£-·àù-·âµ·ä£', '·ä†-·àù-·âµ·ä£', '·ä†-·ä¶-·àù-·âµ·ä£', '·ä†-·âΩ-·àù-·âµ·ä£', '·ä†-·àù·ä£-·àù-·âµ·ä£', '·ä†-·âµ·ä£', '·ä†-·äï-·ä≠·ä†-·àù-·âµ·ä£', '·ä†-·äï-·ä≠·ä°-·àù-·âµ·ä£', '·ä†-·äï-·äΩ-·àù-·âµ·ä£', '·ä†-·äï-·ä≠·ä•-·àù·ä£-·àù-·âµ·ä£', '·ä†-·äï·ä§-·äï-·ä≠·ä†-·àù-·âµ·ä£', '·ä†-·äï·ä§-·äï-·ä≠·ä°-·àù-·âµ·ä£', '·ä†-·äï·ä§-·äï-·äΩ-·àù-·âµ·ä£', '·ä†--·ä†-·äï·ä§-·äï-·ä≠·ä•-·àù·ä£-·àù-·âµ·ä£', '·ä†-·àï·ä†-·äï·ä¢-·àù-·âµ·ä£', '·ä†-·àï·ä†-·äï-·ãµ·ä†-·àù-·âµ·ä£', '·ä†-·àï·ä†-·äï-·àï·ä°-·àù-·âµ·ä£', '·ä†-·àï·ä†-·äï-\\U0001e7e5-·àù-·âµ·ä£', '·ä†-·àï·ä†-·äï-·àï-·àù·ä£-·àù-·âµ·ä£', '·ä†-\\U0001e7eb·ä†-·äï·ä†-·àù-·âµ·ä£', '·ä†-·àï·ä°-·äï·ä¢-·àù-·âµ·ä£', '·ä†-·àï·ä°-·äï-·ãµ·ä†-·àù-·âµ·ä£', '·ä†-·àï·ä°-·äï-·ä≠·ä†-·àù-·âµ·ä£', '·ä†-·àï·ä°-·äï-·ä≠·ä°-·àù-·âµ·ä£', '·ä†-·àï·ä°-·äï-·äΩ-·àù-·âµ·ä£', '·ä†-·àï·ä°-·äï-·ä≠·ä•-·àù·ä£-·àù-·âµ·ä£', '·ä†-·äï·ä¢-·àù-·âµ·ä£', '·ä†-·äï-·ãµ·ä†-·àù-·âµ·ä£', '·ä†-·äï-·àï·ä†-·àù-·âµ·ä£', '·ä†-·äï-·àï·ä°-·àù-·âµ·ä£', '·ä†-·äï-\\U0001e7e5-·àù-·âµ·ä£', '·ä†-·äï-·àï-·àù·ä£-·àù-·âµ·ä£', '·ä†-·ä¶-·äï·ä¢-·àù-·âµ·ä£', '·ä†-·ä¶-·äï-·ãµ·ä†-·àù-·âµ·ä£', '·ä†-·ä¶-·äï-·ä≠·ä†-·àù-·âµ·ä£', '·ä†-·ä¶-·äï-·ä≠·ä°-·àù-·âµ·ä£', '·ä†-·ä¶-·äï-·äΩ-·àù-·âµ·ä£', '·ä†-·ä¶-·äï-·ä≠·ä•-·àù·ä£-·àù-·âµ·ä£', '·ä†-·âΩ·ä†-·äï·ä¢-·àù-·âµ·ä£', '·ä†-·âΩ·ä†-·äï·ä†-·ãµ·ä†-·àù-·âµ·ä£', '·ä†-·âΩ·ä†-·äï·ä†-·àï·ä†-·àù-·âµ·ä£', '·ä†-·âΩ·ä†-·äï·ä†-·àï·ä°-·àù-·âµ·ä£', '·ä†-·âΩ·ä†-·äï·ä†-\\U0001e7e5-·àù-·âµ·ä£', '·ä†-·âΩ·ä†-·äï·ä†-·àï-·àù·ä£-·àù-·âµ·ä£', '·ä†-·âΩ·ä†-·äï·ä†-·àù-·âµ·ä£', '·ä†-·àù·ä£-·äï·ä¢-·àù-·âµ·ä£', '·ä†-·àù·ä£-·äï-·ãµ·ä†-·àù-·âµ·ä£', '·ä†-·àù·ä£-·äï-·àï·ä†-·àù-·âµ·ä£', '·ä†-·àù·ä£-·äï-·àï·ä°-·àù-·âµ·ä£', '·ä†-·àù·ä£-·äï-\\U0001e7e5-·àù-·âµ·ä£', '·ä†-·àù·ä£-·äï-·àï-·àù·ä£-·àù-·âµ·ä£', '·ä†-·àù·ä£-·äï·ä†-·àù-·âµ·ä£', '·ä†-·â•-·ä¢-·àù-·âµ·ä£', '·ä†-·â•-·äï-·ãµ·ä†-·àù-·âµ·ä£', '·ä†-·â•-·àï·ä†-·àù-·âµ·ä£', '·ä†-·â•-·àï·ä°-·àù-·âµ·ä£', '·ä†-·â•-\\U0001e7e5-·àù-·âµ·ä£', '·ä†-·â•-·ä†-·àï-·àù·ä£-·àù-·âµ·ä£', '·ä†-·ãç·ä†-·àù-·âµ·ä£', '·ä†-·â•-·ä¶-·àù-·âµ·ä£', '·ä†-·â•-·ä£-·àù-·âµ·ä£', '·ä†-·â•-·àï-·àù·ä£-·àù-·âµ·ä£', '·ä†-·ä¶-\\U0001e7f7·ä¢-·àù-·âµ·ä£', '·ä†-·ä¶-\\U0001e7f7-·äï-·ãµ·ä†-·àù-·âµ·ä£', '·ä†-·ä¶-\\U0001e7f7·ä†-·àù-·âµ·ä£', '·ä†-·ä¶-·ä≠·ä¶-·àù-·âµ·ä£', '·ä†-·ä¶-·äΩ-·àù-·âµ·ä£', '·ä†-·ä¶-·ä≠·ä•-·àù·ä£-·àù-·âµ·ä£', '·ä†-·ä¶-·ä≠·ä†-·àù-·âµ·ä£', '·ä†-·ä¶-\\U0001e7f7·ä£-·àù-·âµ·ä£', '·ä†-·ä¶-·ä≠·ä†-·àù·ä£-·àù-·âµ·ä£', '·ä†-\\U0001e7eb-·àï·ä†-·àù·ä£', '·ä†-·äï·ä†-·àï·ä†-·àù·ä£', '·ä†-·àï·ä†-·àï·ä†-·àù·ä£', '·ä†-·àï·ä°-·àï·ä†-·àù·ä£', '·ä†-\\U0001e7e5-·àï·ä†-·àù·ä£', '·ä†-·àï-·àù·ä£-·àï·ä†-·àù·ä£', '·ä†-·àï·ä†-·àù·ä£', '·ä†-·ä¶-·àï·ä†-·àù·ä£', '·ä†-·âΩ-·àï·ä†-·àù·ä£', '·ä†-·àù·ä£-·àï·ä†-·àù·ä£', '·ä†-·ä¶-·äï·ä¢-·àï·ä†-·àù·ä£', '·ä†-·ä¶-·äï-·ãµ·ä†-·àï·ä†-·àù·ä£', '·ä†-·ä¶-·äï-·ä≠·ä†-·àï·ä†-·àù·ä£', '·ä†-·ä¶-·äï-·ä≠·ä°-·àï·ä†-·àù·ä£', '·ä†-·ä¶-·äï-·äΩ-·àï·ä†-·àù·ä£', '·ä†-·ä¶-·äï-·ä≠·ä•-·àù·ä£-·àï·ä†-·àù·ä£', '·ä†-·âΩ·ä†-·äï·ä¢-·àï·ä†-·àù·ä£', '·ä†-·âΩ·ä†-·äï·ä†-·ãµ·ä†-·àï·ä†-·àù·ä£', '·ä†-·âΩ·ä†-·äï·ä†-·àï·ä†-·àï·ä†-·àù·ä£', '·ä†-·âΩ·ä†-·äï·ä†-·àï·ä°-·àï·ä†-·àù·ä£', '·ä†-·âΩ·ä†-·äï·ä†-\\U0001e7e5-·àï·ä†-·àù·ä£', '·ä†-·âΩ·ä†-·äï·ä†-·àï-·àù·ä£-·àï·ä†-·àù·ä£', '·ä†-·âΩ·ä†-·äï·ä†-·àï·ä†-·àù·ä£', '·ä†-·àù·ä£-·äï·ä¢-·àï·ä†-·àù·ä£', '·ä†-·àù·ä£-·äï-·ãµ·ä†-·àï·ä†-·àù·ä£', '·ä†-·àù·ä£-·äï-·àï·ä†-·àï·ä†-·àù·ä£', '·ä†-·àù·ä£-·äï-·àï·ä°-·àï·ä†-·àù·ä£', '·ä†-·àù·ä£-·äï-\\U0001e7e5-·àï·ä†-·àù·ä£', '·ä†-·àù·ä£-·äï-·àï-·àù·ä£-·àï·ä†-·àù·ä£', '·ä†-·àù·ä£-·äï·ä†-·àï·ä†-·àù·ä£', '·ä†-·â•-·ä¢-·àï·ä†-·àù·ä£', '·ä†-·â•-·äï-·ãµ·ä†-·àï·ä†-·àù·ä£', '·ä†-·â•-·àï·ä†-·àï·ä†-·àù·ä£', '·ä†-·â•-·àï·ä°-·àï·ä†-·àù·ä£', '·ä†-·â•-\\U0001e7e5-·àï·ä†-·àù·ä£', '·ä†-·â•-·ä†-·àï-·àù·ä£-·àï·ä†-·àù·ä£', '·ä†-·ãç·ä†-·àï·ä†-·àù·ä£', '·ä†-·â•-·ä¶-·àï·ä†-·àù·ä£', '·ä†-·â•-·ä£-·àï·ä†-·àù·ä£', '·ä†-·â•-·àï-·àù·ä£-·àï·ä†-·àù·ä£', '·ä†-·ä¶-\\U0001e7f7·ä¢-·àï·ä†-·àù·ä£', '·ä†-·ä¶-\\U0001e7f7-·äï-·ãµ·ä†-·àï·ä†-·àù·ä£', '·ä†-·ä¶-\\U0001e7f7·ä†-·àï·ä†-·àù·ä£', '·ä†-·ä¶-·ä≠·ä¶-·àï·ä†-·àù·ä£', '·ä†-·ä¶-·äΩ-·àï·ä†-·àù·ä£', '·ä†-·ä¶-·ä≠·ä•-·àù·ä£-·àï·ä†-·àù·ä£', '·ä†-·ä¶-·ä≠·ä†-·àï·ä†-·àù·ä£', '·ä†-·ä¶-\\U0001e7f7·ä£-·àï·ä†-·àù·ä£', '·ä†-·ä¶-·ä≠·ä†-·àù·ä£-·àï·ä†-·àù·ä£', '·ä†-·äï·ä¢-·àï·ä†-·àù·ä£', '·ä†-·äï-·ãµ·ä†-·àï·ä†-·àù·ä£', '·ä†-·äï-·àï·ä†-·àï·ä†-·àù·ä£', '·ä†-·äï-·àï·ä°-·àï·ä†-·àù·ä£', '·ä†-·äï-\\U0001e7e5-·àï·ä†-·àù·ä£', '·ä†-·äï-·àï-·àù·ä£-·àï·ä†-·àù·ä£', '·ä¶']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "file_path = '/content/drive/MyDrive/suffixes removed repeation.txt'\n",
        "\n",
        "# Detect file encoding\n",
        "with open(file_path, 'rb') as file:\n",
        "    raw_data = file.read()\n",
        "    result = chardet.detect(raw_data)\n",
        "\n",
        "# Read prefixes from text file\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    amharic_verb_suffixes = [line.strip() for line in file]\n",
        "\n",
        "print(\"amharic_verb_suffixes:\", amharic_verb_suffixes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM_mW_MMvHML",
        "outputId": "414bf542-1092-46ec-a68f-53180bb49875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amharic_noun_prefixe: ['·ã≠·ä†-', '·âµ·ä†-', '·â•·ä†-', '·ä†-']\n"
          ]
        }
      ],
      "source": [
        "import chardet\n",
        "\n",
        "file_path1 = '/content/drive/MyDrive/Noun_prefixes  filttered.txt'\n",
        "\n",
        "# Detect file encoding\n",
        "with open(file_path1, 'rb') as file:\n",
        "    raw_data = file.read()\n",
        "    result = chardet.detect(raw_data)\n",
        "    encoding = result['encoding']\n",
        "\n",
        "# Read prefixes from text file\n",
        "with open(file_path1, encoding=encoding) as file:\n",
        "    amharic_noun_prefixes = [line.strip() for line in file]\n",
        "\n",
        "print(\"amharic_noun_prefixe:\", amharic_noun_prefixes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2gZSKt4vXGk"
      },
      "outputs": [],
      "source": [
        "file_path2 = '/content/drive/MyDrive/nound_suffixes filtered .txt'\n",
        "# Detect file encoding\n",
        "with open(file_path2, 'rb') as file:\n",
        "    raw_data = file.read()\n",
        "    result = chardet.detect(raw_data)\n",
        "    encoding = result['encoding']\n",
        "\n",
        "# Read prefixes from text file\n",
        "with open(file_path2, 'r', encoding=encoding) as file:\n",
        "    amharic_noun_suffixes = [line.strip() for line in file]\n",
        "\n",
        "print(\"amharic_noun_suffixe:\", amharic_noun_suffixes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9IoL6d-vckZ",
        "outputId": "82200fc0-1980-4255-ed17-b28b398be0a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amharic_adj_prefixe: ['·âµ·ä†-', '·â•·ä†-', '·ã≠·ä†-', '·ä†-']\n"
          ]
        }
      ],
      "source": [
        "import chardet\n",
        "\n",
        "file_path1 = '/content/drive/MyDrive/Adjective_prefixes filttered.txt'\n",
        "\n",
        "# Detect file encoding\n",
        "with open(file_path1, 'rb') as file:\n",
        "    raw_data = file.read()\n",
        "    result = chardet.detect(raw_data)\n",
        "    encoding = result['encoding']\n",
        "\n",
        "# Read prefixes from text file\n",
        "with open(file_path1, encoding=encoding) as file:\n",
        "    amharic_adj_prefixes = [line.strip() for line in file]\n",
        "\n",
        "print(\"amharic_adj_prefixe:\", amharic_adj_prefixes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ3o4XGyvoRu"
      },
      "outputs": [],
      "source": [
        "file_path2 = '/content/drive/MyDrive/Adjective_suffixes  filttered.txt'\n",
        "# Detect file encoding\n",
        "with open(file_path2, 'rb') as file:\n",
        "    raw_data = file.read()\n",
        "    result = chardet.detect(raw_data)\n",
        "    encoding = result['encoding']\n",
        "\n",
        "# Read prefixes from text file\n",
        "with open(file_path2, 'r', encoding=encoding) as file:\n",
        "    amharic_adj_suffixes = [line.strip() for line in file]\n",
        "\n",
        "print(\"amharic_adj_suffixes:\", amharic_adj_suffixes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwjDVWCQvoKl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4V7qgjEvp_K"
      },
      "outputs": [],
      "source": [
        "def load_dictionary():\n",
        "    with open('/content/drive/MyDrive/Noun_root.txt', 'r') as f:\n",
        "        noun_list.extend(f.read().splitlines())\n",
        "    with open('/content/drive/MyDrive/list of verbs_root.txt', 'r') as f:\n",
        "        verb_list.extend(f.read().splitlines())\n",
        "    with open('/content/drive/MyDrive/Adjective_root.txt', 'r') as f:\n",
        "        adjective_list.extend(f.read().splitlines())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHKIt_trvv9U"
      },
      "outputs": [],
      "source": [
        "# Check if a word is in the noun list\n",
        "def is_noun(word):\n",
        "    return word in noun_list\n",
        "\n",
        "# Check if a word is in the verb list\n",
        "def is_verb(word):\n",
        "    return word in verb_list\n",
        "\n",
        "# Check if a word is in the adjective list\n",
        "def is_adjective(word):\n",
        "    return word in adjective_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEFE75xdv7IV"
      },
      "outputs": [],
      "source": [
        "def read_amharic_prefixes_suffixes(file_path1, file_path2):\n",
        "    amharic_prefixes = []\n",
        "    amharic_suffixes = []\n",
        "\n",
        "    # Detect file encoding for prefixes\n",
        "    with open(file_path1, 'rb') as file:\n",
        "        raw_data = file.read()\n",
        "        result = chardet.detect(raw_data)\n",
        "        encoding = result['encoding']\n",
        "\n",
        "    # Read prefixes from text file\n",
        "    with open(file_path1, encoding=encoding) as file:\n",
        "        amharic_prefixes = [line.strip() for line in file]\n",
        "\n",
        "    # Detect file encoding for suffixes\n",
        "    with open(file_path2, 'rb') as file:\n",
        "      raw_data = file.read()\n",
        "      result = chardet.detect(raw_data)\n",
        "      # Read prefixes from text file\n",
        "    with open(file_path2, 'r', encoding='utf-8') as file:\n",
        "      amharic_suffixes = [line.strip() for line in file]\n",
        "    return amharic_prefixes, amharic_suffixes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVekgProisTS"
      },
      "outputs": [],
      "source": [
        "def remove_amharic_noun_affix(word):\n",
        "    for prefix in amharic_noun_prefixes:\n",
        "        if word.startswith(prefix):\n",
        "            word = word[len(prefix):]\n",
        "            break\n",
        "    for suffix in amharic_noun_suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            word = word[:-len(suffix)]\n",
        "            break\n",
        "    return word#.rstrip(word[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzWq-4jMxbM7"
      },
      "outputs": [],
      "source": [
        "def remove_amharic_verb_affix(word):\n",
        "    for prefix in amharic_verb_prefixes:\n",
        "        if word.startswith(prefix):\n",
        "            word = word[len(prefix):]\n",
        "            break\n",
        "    for suffix in amharic_verb_suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            word = word[:-len(suffix)]\n",
        "            break\n",
        "    return word#.rstrip(word[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FE0GnCBLxgGj"
      },
      "outputs": [],
      "source": [
        "def remove_amharic_Adj_affix(word):\n",
        "    for prefix in amharic_adj_prefixes:\n",
        "        if word.startswith(prefix):\n",
        "            word = word[len(prefix):]\n",
        "            break\n",
        "    for suffix in amharic_adj_suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            word = word[:-len(suffix)]\n",
        "            break\n",
        "    return word#.rstrip(word[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT7ROv4LPadp"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def find_closest_word(root_word, misspelled_word):\n",
        "    with open(root_word, \"r\", encoding=\"utf-8\") as file:\n",
        "        root_words = [line.strip() for line in file]\n",
        "\n",
        "    closest_word = difflib.get_close_matches(misspelled_word, root_words, n=1)\n",
        "    if closest_word:\n",
        "        return closest_word[0]\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_amharic_paragraph(paragraph):\n",
        "    words = paragraph.split()\n",
        "    return words"
      ],
      "metadata": {
        "id": "RX1Tcc9GZTPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_amharic_word(word):\n",
        "    amharic_alphabet = am  # Add all Amharic characters to the list\n",
        "    for char in word:\n",
        "        if char not in amharic_alphabet:\n",
        "            return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "khvetm-EgBib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "\n",
        "# Download the Amharic tokenizer resources from NLTK\n",
        "nltk.download('punkt')\n",
        "\n",
        "def remove_last_character(line):\n",
        "    return line[:-1]\n",
        "\n",
        "def is_amharic_word(word):\n",
        "    amharic_alphabet = [...]  # Add all Amharic characters to the list\n",
        "    for char in word:\n",
        "        if char not in amharic_alphabet:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def tokenize_amharic_paragraph(paragraph):\n",
        "    words = nltk.word_tokenize(paragraph)\n",
        "    return words\n",
        "\n",
        "def remove_punctuation(word):\n",
        "    amharic_punctuation = \"·ç¢'\\\"·ç£·ç§·ç•.·ç¶·çß·ç®[]=-_{}\"\n",
        "    translator = str.maketrans(\"\", \"\", string.punctuation + amharic_punctuation)\n",
        "    word_without_punct = word.translate(translator)\n",
        "    return word_without_punct\n",
        "\n",
        "# Prompt the user to enter the Amharic paragraph\n",
        "amharic_paragraph = input(\"Enter the Amharic paragraph: \")\n",
        "\n",
        "# Tokenize the Amharic paragraph\n",
        "\n",
        "amharic_words = tokenize_amharic_paragraph(amharic_paragraph)\n",
        "print(amharic_words)\n",
        "# Remove punctuation from the Amharic words\n",
        "amharic_words_without_punct = [remove_punctuation(word) for word in amharic_words]\n",
        "print(amharic_words_without_punct)\n",
        "amharic_words_without_punct = [word for word in amharic_words_without_punct if word]  # Remove empty words\n",
        "print(amharic_words_without_punct)\n",
        "# amharic_words_without_punct is the list of Amharic words without punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGaAk18waKDJ",
        "outputId": "a866d711-96a8-4aa9-da2f-018c31081443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the Amharic paragraph: ·â∏·äê\n",
            "['·â∏·äê']\n",
            "['·â∏·äê']\n",
            "['·â∏·äê']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vAXQQx79fbrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5B3bRUZixAz",
        "outputId": "0f8f408f-b2da-49ce-8691-632ef3523c37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amharic_paragrap['·â∏·äê','·â∏·äêûü´·àù','·â∏·äê·äê·àù','·â∏·äê·àê·àù','·â∏·äê·àë·àù','·â∏·äêûü•·àù','·â∏·äê·àï·àõ·àù','·â∏·äê·ä†·àù','·â∏·äê·àù','·â∏·äê·ä¶·àù','·â∏·äê·âΩ·àù','·â∏·äê·àõ·àù','·â∏·äê·äï·ä®·àù']\n",
            "The string '·â∏·äê'noun'' is \u001b[32mpresent\u001b[0m in the file.\n",
            "The string '·â∏·äêûü´·àù' is \u001b[32mpresent\u001b[0m in the file.\n",
            "The string '·â∏·äê·äê·àù' is \u001b[32mpresent\u001b[0m in the file.\n",
            "The string '·â∏·äê·àê·àù'noun'' is \u001b[32mpresent\u001b[0m in the file.\n",
            "The string '·â∏·äê·àë·àù'noun'' is \u001b[32mpresent\u001b[0m in the file.\n",
            "The string '·â∏·äêûü•·àù'noun'' is \u001b[32mpresent\u001b[0m in the file.\n",
            "The string '·â∏·äê·àï·àõ·àù'noun'' is \u001b[32mpresent\u001b[0m in the file.\n",
            "The string '·â∏·äê·ä†·àù' is \u001b[32mpresent\u001b[0m in the file.\n",
            "The string '·â∏·äê·àù'noun'' is \u001b[32mpresent\u001b[0m in the file.\n",
            "The string '·â∏·äê·ä¶·àù' is \u001b[32mpresent\u001b[0m in the file.\n",
            "The string '·â∏·äê·âΩ·àù' is \u001b[32mpresent\u001b[0m in the file.\n",
            "The string '·â∏·äê·àõ·àù'noun'' is \u001b[32mpresent\u001b[0m in the file.\n",
            "The string '·â∏·äê·äï·ä®·àù' is \u001b[31mnot found\u001b[0m in the file.\n",
            "The spelling of '·â∏·äê·äï·ä®·àù' \u001b[31mis incorrect. Did you mean: \u001b[32m['·ã®·â∏·äê', '·ä£·â∏·äê', '·ä£·äì·â∏·äê', '·ä†·â∏·äê', '·ä£·äï·â∏·äê', '·â†·â∏·äê', '·â£·äï·â∏·äê', '·â£·â∏·äê', '·â£·äì·â∏·äê', '·â≥·â∏·äê']\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import difflib\n",
        "import os\n",
        "import glob\n",
        "from colorama import Fore, Style\n",
        "import re\n",
        "#word2 = ['·â∏·äê·âÄ·àù']#'·â∏·äê', '·â∏·äê·å•·àù',\n",
        "word2 =['·â∏·äê','·â∏·äêûü´·àù','·â∏·äê·äê·àù','·â∏·äê·àê·àù','·â∏·äê·àë·àù','·â∏·äêûü•·àù','·â∏·äê·àï·àõ·àù','·â∏·äê·ä†·àù','·â∏·äê·àù','·â∏·äê·ä¶·àù','·â∏·äê·âΩ·àù','·â∏·äê·àõ·àù','·â∏·äê·äï·ä®·àù']\n",
        "count_present = 0\n",
        "count_not_present = 0\n",
        "distances_dict = {}\n",
        "noun_list=[]\n",
        "verb_list=[]\n",
        "adjective_list=[]\n",
        "amharic_paragraph = input(\"amharic_paragrap\")\n",
        "amharic_words = tokenize_amharic_paragraph(amharic_paragraph)\n",
        "amharic_words_without_punct = [remove_punctuation(word) for word in amharic_words]\n",
        "amharic_words_without_punct = [word for word in amharic_words_without_punct if word]  # Remove empty words\n",
        "word2=amharic_words_without_punct\n",
        "for w in word2:\n",
        "    newam = Erbata_TIGBERA(w, RIBI)\n",
        "    newam1 = remove_amharic_noun_affix(newam)\n",
        "    newam2 = remove_amharic_verb_affix(newam)\n",
        "    newam3 = remove_amharic_Adj_affix(newam)\n",
        "    amharic_word = w\n",
        "    search_string1 = newam1\n",
        "    search_string2 = newam2\n",
        "    search_string3 = newam3\n",
        "\n",
        "    RESET = '\\033[0m'\n",
        "    GREEN = '\\033[32m'\n",
        "    RED = '\\033[31m'\n",
        "\n",
        "    if check_string_in_file(file_path, search_string1):\n",
        "        catagory='noun'\n",
        "        search_string = search_string1\n",
        "        steam_form = so_called_steam_noun_word(newam1)\n",
        "        print(f\"The string '{remove_hyphen(amharic_word)}'{catagory}'' is {GREEN}present{RESET} in the file.\")\n",
        "        count_present += 1\n",
        "    elif check_string_in_file(file_path, search_string2):\n",
        "        catagory=''\n",
        "        search_string = search_string2\n",
        "        steam_form = so_called_steam_verb_word(newam2)\n",
        "        print(f\"The string '{remove_hyphen(amharic_word)}' is {GREEN}present{RESET} in the file.\")\n",
        "        count_present += 1\n",
        "    elif check_string_in_file(file_path, search_string3):\n",
        "        search_string = search_string1\n",
        "        steam_form = so_called_steam_adj_word(newam3)\n",
        "        print(f\"The string '{remove_hyphen(amharic_word)}' is {GREEN}present{RESET} in the file.\")\n",
        "        count_present += 1\n",
        "    else:\n",
        "        print(f\"The string '{remove_hyphen(amharic_word)}' is {RED}not found{RESET} in the file.\")\n",
        "        count_not_present += 1\n",
        "\n",
        "        closest1 = find_closest_word('/content/drive/MyDrive/all_root.txt', search_string)\n",
        "        closest = reverse_Erbata(closest1, RIBI)\n",
        "        load_dictionary()\n",
        "        if is_noun(closest1):\n",
        "          close_Aray = add_amharic_noun_affixes(closest1)\n",
        "        elif is_verb(closest1):\n",
        "          close_Aray = add_amharic_verb_affixes(closest1)\n",
        "        elif is_adjective(closest1):\n",
        "          close_Aray = add_amharic_Adj_affixes(closest1)\n",
        "        close_Aray_re = []\n",
        "        close_Aray_re_removed_phe = []\n",
        "        avaragedist = []\n",
        "\n",
        "        for word in close_Aray:\n",
        "            manipulated_word = reverse_Erbata(word, RIBI)\n",
        "            removed_phe = remove_hyphen(manipulated_word)\n",
        "            close_Aray_re.append(manipulated_word)\n",
        "            close_Aray_re_removed_phe.append(removed_phe)\n",
        "\n",
        "        if closest:\n",
        "            print(f\"The spelling of '{w}' {Fore.RED}is incorrect. Did you mean: {Fore.GREEN}{close_Aray_re_removed_phe[:10]}{Style.RESET_ALL}\")\n",
        "            word1 = w\n",
        "\n",
        "            for word in close_Aray_re_removed_phe:\n",
        "                matched, unmatched1_before, unmatched_chars2_before, unmatched1_after, unmatched_chars2_after, total_distance, comparisons, average_distance = compare_words(word1, word)\n",
        "\n",
        "                avaragedist.append(average_distance)\n",
        "                distances_dict[w] = average_distance\n",
        "                distances_dict[w] = float(re.sub(\"\\x1b\\[.*?m\", \"\", average_distance))\n",
        "        else:\n",
        "            print(f\"No close match found for '{search_string}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QgVY6RRA6W2",
        "outputId": "e654c230-da59-442e-d329-7594a3621e81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distances Dictionary: {'·â∏·äê·äï·ä®·àù': 0.4}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "sorted_distances = sorted(avaragedist, key=lambda dist: float(re.sub(\"\\x1b\\[.*?m\", \"\", dist)))\n",
        "peak_value = float(re.sub(\"\\x1b\\[.*?m\", \"\", sorted_distances[-1]))\n",
        "\n",
        "\n",
        "print(\"Distances Dictionary:\", distances_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF6iAvRA7PPC",
        "outputId": "d17e6ab6-4f3c-4b30-fad9-447ac3576de7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison between ·â∏·äê·äï·ä®·àù and ·â∏·äê\n",
            "unmatched1_before_match: ·â∏·äê is: \n",
            "unmatched1_after_match: ·â∏·äê is: ·äï·ä®·àù\n",
            "Matched characters: ·â∏·äê\n",
            "unmatched2_before_match: ·â∏·äê is: \n",
            "unmatched2_After_match: ·â∏·äê is: \n",
            "Total distance: 2.3\n",
            "Comparisons: 5\n",
            "Average distance: \u001b[91m0.45999999999999996\u001b[0m\n",
            "\n",
            "Comparison between ·â∏·äê·äï·ä®·àù and ·â∏·äêûü´·àù\n",
            "unmatched1_before_match: ·â∏·äê·àù is: \n",
            "unmatched1_after_match: ·â∏·äê·àù is: ·ä®·àù\n",
            "Matched characters: ·â∏·äê·àù\n",
            "unmatched2_before_match: ·â∏·äê·àù is: \n",
            "unmatched2_After_match: ·â∏·äê·àù is: ·àù\n",
            "Total distance: 3.4\n",
            "Comparisons: 5\n",
            "Average distance: \u001b[91m0.6799999999999999\u001b[0m\n",
            "\n",
            "Comparison between ·â∏·äê·äï·ä®·àù and ·â∏·äê·äê·àù\n",
            "unmatched1_before_match: ·â∏·äê·äê·àù is: \n",
            "unmatched1_after_match: ·â∏·äê·äê·àù is: ·àù\n",
            "Matched characters: ·â∏·äê·äê·àù\n",
            "unmatched2_before_match: ·â∏·äê·äê·àù is: \n",
            "unmatched2_After_match: ·â∏·äê·äê·àù is: \n",
            "Total distance: 4.1\n",
            "Comparisons: 5\n",
            "Average distance: \u001b[91m0.82\u001b[0m\n",
            "\n",
            "Comparison between ·â∏·äê·äï·ä®·àù and ·â∏·äê·àê·àù\n",
            "unmatched1_before_match: ·â∏·äê·àù is: \n",
            "unmatched1_after_match: ·â∏·äê·àù is: ·ä®·àù\n",
            "Matched characters: ·â∏·äê·àù\n",
            "unmatched2_before_match: ·â∏·äê·àù is: \n",
            "unmatched2_After_match: ·â∏·äê·àù is: ·àù\n",
            "Total distance: 3.4\n",
            "Comparisons: 5\n",
            "Average distance: \u001b[91m0.6799999999999999\u001b[0m\n",
            "\n",
            "Comparison between ·â∏·äê·äï·ä®·àù and ·â∏·äê·àë·àù\n",
            "unmatched1_before_match: ·â∏·äê·àù is: \n",
            "unmatched1_after_match: ·â∏·äê·àù is: ·ä®·àù\n",
            "Matched characters: ·â∏·äê·àù\n",
            "unmatched2_before_match: ·â∏·äê·àù is: \n",
            "unmatched2_After_match: ·â∏·äê·àù is: ·àù\n",
            "Total distance: 3.4\n",
            "Comparisons: 5\n",
            "Average distance: \u001b[91m0.6799999999999999\u001b[0m\n",
            "\n",
            "Comparison between ·â∏·äê·äï·ä®·àù and ·â∏·äêûü•·àù\n",
            "unmatched1_before_match: ·â∏·äê·àù is: \n",
            "unmatched1_after_match: ·â∏·äê·àù is: ·ä®·àù\n",
            "Matched characters: ·â∏·äê·àù\n",
            "unmatched2_before_match: ·â∏·äê·àù is: \n",
            "unmatched2_After_match: ·â∏·äê·àù is: ·àù\n",
            "Total distance: 3.4\n",
            "Comparisons: 5\n",
            "Average distance: \u001b[91m0.6799999999999999\u001b[0m\n",
            "\n",
            "Comparison between ·â∏·äê·äï·ä®·àù and ·â∏·äê·àï·àõ·àù\n",
            "unmatched1_before_match: ·â∏·äê·àù is: \n",
            "unmatched1_after_match: ·â∏·äê·àù is: ·ä®·àù\n",
            "Matched characters: ·â∏·äê·àù\n",
            "unmatched2_before_match: ·â∏·äê·àù is: \n",
            "unmatched2_After_match: ·â∏·äê·àù is: ·àõ·àù\n",
            "Total distance: 5.0\n",
            "Comparisons: 7\n",
            "Average distance: \u001b[91m0.7142857142857143\u001b[0m\n",
            "\n",
            "Comparison between ·â∏·äê·äï·ä®·àù and ·â∏·äê·ä†·àù\n",
            "unmatched1_before_match: ·â∏·äê·àù is: \n",
            "unmatched1_after_match: ·â∏·äê·àù is: ·ä®·àù\n",
            "Matched characters: ·â∏·äê·àù\n",
            "unmatched2_before_match: ·â∏·äê·àù is: \n",
            "unmatched2_After_match: ·â∏·äê·àù is: ·àù\n",
            "Total distance: 3.4\n",
            "Comparisons: 5\n",
            "Average distance: \u001b[91m0.6799999999999999\u001b[0m\n",
            "\n",
            "Comparison between ·â∏·äê·äï·ä®·àù and ·â∏·äê·àù\n",
            "unmatched1_before_match: ·â∏·äê·àù is: \n",
            "unmatched1_after_match: ·â∏·äê·àù is: ·ä®·àù\n",
            "Matched characters: ·â∏·äê·àù\n",
            "unmatched2_before_match: ·â∏·äê·àù is: \n",
            "unmatched2_After_match: ·â∏·äê·àù is: \n",
            "Total distance: 3.2\n",
            "Comparisons: 5\n",
            "Average distance: \u001b[91m0.64\u001b[0m\n",
            "\n",
            "Comparison between ·â∏·äê·äï·ä®·àù and ·â∏·äê·ä¶·àù\n",
            "unmatched1_before_match: ·â∏·äê·àù is: \n",
            "unmatched1_after_match: ·â∏·äê·àù is: ·ä®·àù\n",
            "Matched characters: ·â∏·äê·àù\n",
            "unmatched2_before_match: ·â∏·äê·àù is: \n",
            "unmatched2_After_match: ·â∏·äê·àù is: ·àù\n",
            "Total distance: 3.4\n",
            "Comparisons: 5\n",
            "Average distance: \u001b[91m0.6799999999999999\u001b[0m\n",
            "\n",
            "Comparison between ·â∏·äê·äï·ä®·àù and ·â∏·äê·âΩ·àù\n",
            "unmatched1_before_match: ·â∏·äê·àù is: \n",
            "unmatched1_after_match: ·â∏·äê·àù is: ·ä®·àù\n",
            "Matched characters: ·â∏·äê·àù\n",
            "unmatched2_before_match: ·â∏·äê·àù is: \n",
            "unmatched2_After_match: ·â∏·äê·àù is: ·àù\n",
            "Total distance: 3.4\n",
            "Comparisons: 5\n",
            "Average distance: \u001b[91m0.6799999999999999\u001b[0m\n",
            "\n",
            "Comparison between ·â∏·äê·äï·ä®·àù and ·â∏·äê·àõ·àù\n",
            "unmatched1_before_match: ·â∏·äê·àù is: \n",
            "unmatched1_after_match: ·â∏·äê·àù is: ·ä®·àù\n",
            "Matched characters: ·â∏·äê·àù\n",
            "unmatched2_before_match: ·â∏·äê·àù is: \n",
            "unmatched2_After_match: ·â∏·äê·àù is: ·àù\n",
            "Total distance: 3.4\n",
            "Comparisons: 5\n",
            "Average distance: \u001b[91m0.6799999999999999\u001b[0m\n",
            "\n",
            "Comparison between ·â∏·äê·äï·ä®·àù and ·â∏·äê·äï·ä®·àù\n",
            "unmatched1_before_match: ·â∏·äê·äï·ä®·àù is: \n",
            "unmatched1_after_match: ·â∏·äê·äï·ä®·àù is: \n",
            "Matched characters: ·â∏·äê·äï·ä®·àù\n",
            "unmatched2_before_match: ·â∏·äê·äï·ä®·àù is: \n",
            "unmatched2_After_match: ·â∏·äê·äï·ä®·àù is: \n",
            "Total distance: 5.0\n",
            "Comparisons: 5\n",
            "Average distance: \u001b[91m1.0\u001b[0m\n",
            "\n",
            "Distances Dictionary: {'·â∏·äê': 0.45999999999999996, '·â∏·äê\\U0001e7eb·àù': 0.6799999999999999, '·â∏·äê·äê·àù': 0.82, '·â∏·äê·àê·àù': 0.6799999999999999, '·â∏·äê·àë·àù': 0.6799999999999999, '·â∏·äê\\U0001e7e5·àù': 0.6799999999999999, '·â∏·äê·àï·àõ·àù': 0.7142857142857143, '·â∏·äê·ä†·àù': 0.6799999999999999, '·â∏·äê·àù': 0.64, '·â∏·äê·ä¶·àù': 0.6799999999999999, '·â∏·äê·âΩ·àù': 0.6799999999999999, '·â∏·äê·àõ·àù': 0.6799999999999999, '·â∏·äê·äï·ä®·àù': 1.0}\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "distances_dict = {}\n",
        "\n",
        "for w in word2:\n",
        "    matched, unmatched1_before, unmatched_chars2_before, unmatched1_after, unmatched_chars2_after, total_distance, comparisons, average_distance = compare_words(word1, w)\n",
        "    print(\"Comparison between\", word1, \"and\", w)\n",
        "    print(\"unmatched1_before_match:\", matched, \"is:\", unmatched1_before)\n",
        "    print(\"unmatched1_after_match:\", matched, \"is:\", unmatched1_after)\n",
        "    print(\"Matched characters:\", matched)\n",
        "    print(\"unmatched2_before_match:\", matched, \"is:\", unmatched_chars2_before)\n",
        "    print(\"unmatched2_After_match:\", matched, \"is:\", unmatched_chars2_after)\n",
        "    print(\"Total distance:\", total_distance)\n",
        "    print(\"Comparisons:\", comparisons)\n",
        "    print(\"Average distance:\", average_distance)\n",
        "    avaragedist.append(average_distance)\n",
        "    distances_dict[w] = average_distance\n",
        "    distances_dict[w] = float(re.sub(\"\\x1b\\[.*?m\", \"\", average_distance))\n",
        "    print()\n",
        "sorted_distances = sorted(avaragedist, key=lambda dist: float(re.sub(\"\\x1b\\[.*?m\", \"\", dist)))\n",
        "peak_value = float(re.sub(\"\\x1b\\[.*?m\", \"\", sorted_distances[-1]))\n",
        "\n",
        "print(\"Distances Dictionary:\", distances_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr1X12da7Vaj",
        "outputId": "ef5f670b-11e5-4bb4-9540-ab02ca5ae351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distances Dictionary: {'·â∏·äê': 0.45999999999999996, '·â∏·äê\\U0001e7eb·àù': 0.6799999999999999, '·â∏·äê·äê·àù': 0.82, '·â∏·äê·àê·àù': 0.6799999999999999, '·â∏·äê·àë·àù': 0.6799999999999999, '·â∏·äê\\U0001e7e5·àù': 0.6799999999999999, '·â∏·äê·àï·àõ·àù': 0.7142857142857143, '·â∏·äê·ä†·àù': 0.6799999999999999, '·â∏·äê·àù': 0.64, '·â∏·äê·ä¶·àù': 0.6799999999999999, '·â∏·äê·âΩ·àù': 0.6799999999999999, '·â∏·äê·àõ·àù': 0.6799999999999999, '·â∏·äê·äï·ä®·àù': 1.0}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "sorted_distances = sorted(avaragedist, key=lambda dist: float(re.sub(\"\\x1b\\[.*?m\", \"\", dist)))\n",
        "peak_value = float(re.sub(\"\\x1b\\[.*?m\", \"\", sorted_distances[-1]))\n",
        "\n",
        "\n",
        "print(\"Distances Dictionary:\", distances_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B1Ou_i5XGibZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rena-I0opc-o",
        "tlegbBLS_49L",
        "KBi2opVBqJu9",
        "tbGztI1ipzeY",
        "GATlxdAmqXGE",
        "dt7lGInCqlmp",
        "Bdx5asePrdXz",
        "DPnH4Q0Rrtf-",
        "tnBcNS3xrzSv",
        "wDgSBKg1sQ4n",
        "U7ULOWofsjXI",
        "qf_j8vCgtJE9",
        "uWQ9NU1GtYLc"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}